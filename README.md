# BERT_Links

#
https://web.stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html
https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be
https://hal.archives-ouvertes.fr/hal-03038776/document
https://compneuro.neuromatch.io/tutorials/intro.html

#
https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1

#
https://docs.wandb.ai/ref/app/features/panels/weave/embedding-projector

#
http://web.stanford.edu/class/cs124/

#
https://www.eecs.yorku.ca/~kosta/Courses/EECS4422-F21/index.html

#
https://franknielsen.github.io/GSI/

#
https://nlp.stanford.edu/~johnhew/vocab-expansion.html

#
https://www.youtube.com/watch?v=-NBvRNPRzTo

#
https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n
https://stackoverflow.com/questions/37095783/how-is-a-convolution-calculated-on-an-image-with-three-rgb-channels?noredirect=1&lq=1
https://stackoverflow.com/questions/54727606/how-do-convolutional-layers-cnns-work-in-keras?noredirect=1&lq=1
https://stackoverflow.com/questions/65275805/how-to-implement-two-layers-of-keras-conv1d-in-numpy?rq=1

#
https://jaketae.github.io/study/relative-positional-encoding/?utm_content=190320977&utm_medium=social&utm_source=linkedin&hss_channel=lcp-42461735

#
https://kazemnejad.com/blog/transformer_architecture_positional_encoding/

#
https://jalammar.github.io/illustrated-transformer/
https://e2eml.school/transformers.html
https://nlp.seas.harvard.edu/2018/04/03/attention.html

#
https://colah.github.io/posts/2015-09-Visual-Information/
https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained
